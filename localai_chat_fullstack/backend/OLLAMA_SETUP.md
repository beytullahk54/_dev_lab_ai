# ğŸ¦™ Ollama ile LLM KullanÄ±mÄ±

## ğŸ“‹ Gereksinimler

### 1. Ollama'yÄ± YÃ¼kleyin
Windows iÃ§in: https://ollama.ai/download

### 2. Ollama Servisini BaÅŸlatÄ±n
```bash
ollama serve
```

### 3. Gemma 2:2b Modelini Ä°ndirin
```bash
ollama pull gemma2:2b
```

**Not:** EÄŸer `gemma3:4b` kullanmak isterseniz:
```bash
ollama pull gemma3:4b
```

Sonra `main.py` dosyasÄ±nda `MODEL_NAME` deÄŸiÅŸkenini deÄŸiÅŸtirin:
```python
MODEL_NAME = "gemma3:4b"  # veya "gemma2:2b"
```

## ğŸš€ Backend'i BaÅŸlatÄ±n

```bash
cd backend
python main.py
```

## âœ… Test Edin

1. **API Durumu:** http://localhost:8000
2. **Swagger Docs:** http://localhost:8000/docs
3. **Streaming Test:** Frontend'i Ã§alÄ±ÅŸtÄ±rÄ±n

## ğŸ“Š Mevcut Modelleri GÃ¶rÃ¼ntÃ¼leyin

```bash
ollama list
```

## ğŸ”„ DeÄŸiÅŸiklikler

### Ã–nceki Sistem (Hugging Face):
- âŒ YavaÅŸ (CPU'da Ã§alÄ±ÅŸÄ±yor)
- âŒ BÃ¼yÃ¼k baÄŸÄ±mlÄ±lÄ±klar (torch, transformers)
- âŒ Model indirme karmaÅŸÄ±k

### Yeni Sistem (Ollama):
- âœ… HÄ±zlÄ± (optimize edilmiÅŸ)
- âœ… Hafif baÄŸÄ±mlÄ±lÄ±klar
- âœ… Kolay model yÃ¶netimi
- âœ… GerÃ§ek streaming desteÄŸi

## ğŸ¯ Avantajlar

1. **GerÃ§ek Streaming:** Token'lar gerÃ§ek zamanlÄ± Ã¼retilir
2. **Daha HÄ±zlÄ±:** Ollama optimize edilmiÅŸ inference saÄŸlar
3. **Kolay YÃ¶netim:** `ollama pull`, `ollama list` gibi komutlar
4. **Daha Ä°yi Modeller:** Gemma, Llama, Mistral vb. kolayca kullanÄ±labilir
