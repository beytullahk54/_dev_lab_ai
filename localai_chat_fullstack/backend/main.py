from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn
import asyncio
import json
from agents.run import app as agent_workflow

# FastAPI uygulamasÄ±
app = FastAPI(title="Simple LLM API", version="1.0.0")

# CORS ayarlarÄ± (frontend ile iletiÅŸim iÃ§in)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # GeliÅŸtirme iÃ§in tÃ¼m originlere izin ver
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Ollama ile LLM kullanÄ±mÄ±
LLM_LOADED = False
MODEL_NAME = "gemma3:4b"  # Ollama'da yÃ¼klÃ¼ model adÄ±

try:
    import ollama
    print(f"Ollama baÄŸlantÄ±sÄ± kontrol ediliyor...")
    print(f"Model: {MODEL_NAME}")
    
    # Ollama'nÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol et - basit bir test isteÄŸi gÃ¶nder
    try:
        models_response = ollama.list()
        # models_response bir dict, iÃ§inde 'models' key'i var ve bu bir liste
        if 'models' in models_response:
            model_names = [m.get('name', m.get('model', '')) for m in models_response['models']]
            print(f"Mevcut modeller: {model_names}")
            
            # Model adÄ±nÄ± kontrol et (tam eÅŸleÅŸme veya :latest ile)
            if MODEL_NAME in model_names or f"{MODEL_NAME}:latest" in model_names:
                LLM_LOADED = True
                print(f"âœ… Model '{MODEL_NAME}' hazÄ±r!")
            else:
                print(f"âš ï¸  Model '{MODEL_NAME}' bulunamadÄ±.")
                print(f"ğŸ“¥ YÃ¼klemek iÃ§in: ollama pull {MODEL_NAME}")
        else:
            print("âš ï¸  Ollama Ã§alÄ±ÅŸÄ±yor ama model listesi alÄ±namadÄ±")
            
    except Exception as list_error:
        print(f"âš ï¸  Model listesi alÄ±namadÄ±: {list_error}")
        # Yine de modeli kullanmayÄ± dene
        LLM_LOADED = True
        print(f"Model '{MODEL_NAME}' kullanÄ±lmaya Ã§alÄ±ÅŸÄ±lacak...")
        
except Exception as e:
    print(f"âŒ Ollama baÄŸlantÄ±sÄ± baÅŸarÄ±sÄ±z: {e}")
    print("Demo modunda Ã§alÄ±ÅŸÄ±yor...")
    print("Ollama'yÄ± baÅŸlatmak iÃ§in: ollama serve")

# Request modeli
class QuestionRequest(BaseModel):
    question: str
    max_length: int = 100

# Response modeli
class AnswerResponse(BaseModel):
    question: str
    answer: str
    model_loaded: bool

@app.get("/")
async def root():
    """API durumunu kontrol et"""
    return {
        "status": "running",
        "message": "LLM API Ã§alÄ±ÅŸÄ±yor",
        "model": MODEL_NAME if LLM_LOADED else "demo",
        "model_loaded": LLM_LOADED
    }

@app.post("/ask/stream")
async def ask_question_stream(request: QuestionRequest):
    """
    LLM'e soru sor ve cevabÄ± chunk chunk gÃ¶nder (streaming)
    
    Args:
        request: Soru ve maksimum cevap uzunluÄŸu
    
    Returns:
        Server-Sent Events formatÄ±nda streaming cevap
    """
    async def generate_response():
        try:
            if LLM_LOADED:
                # Agent workflow'u Ã§alÄ±ÅŸtÄ±r (streaming deÄŸil, tek seferde sonuÃ§ dÃ¶ner)
                # invoke() senkron olduÄŸu iÃ§in thread pool'da Ã§alÄ±ÅŸtÄ±rÄ±yoruz
                result = await asyncio.to_thread(
                    agent_workflow.invoke, 
                    {"user_query": request.question, "intent": "", "final_answer": ""}
                )
                
                # Sonucu al
                answer = result.get('final_answer', 'Cevap bulunamadÄ±.')
                
                # CevabÄ± kelime kelime stream et (gerÃ§ek streaming simÃ¼lasyonu)
                words = answer.split()
                for i, word in enumerate(words):
                    chunk_data = {
                        "chunk": word + " ",
                        "done": i == len(words) - 1
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
                    await asyncio.sleep(0.05)  # Streaming efekti iÃ§in kÃ¼Ã§Ã¼k gecikme
                    
            else:
                # Demo cevap
                answer = f"{request.question}\n\nBu bir demo cevaptÄ±r. Ollama baÄŸlantÄ±sÄ± yok."
                
                # CevabÄ± kelime kelime gÃ¶nder
                words = answer.split()
                for i, word in enumerate(words):
                    chunk_data = {
                        "chunk": word + " ",
                        "done": i == len(words) - 1
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
                    await asyncio.sleep(0.05)
            
            # Son chunk - iÅŸlem tamamlandÄ±
            yield f"data: {json.dumps({'chunk': '', 'done': True})}\n\n"
            
        except Exception as e:
            error_data = {"error": str(e), "done": True}
            yield f"data: {json.dumps(error_data)}\n\n"
    
    return StreamingResponse(
        generate_response(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

@app.get("/health")
async def health_check():
    """SaÄŸlÄ±k kontrolÃ¼"""
    return {
        "status": "healthy",
        "model_loaded": LLM_LOADED
    }

if __name__ == "__main__":
    # Sunucuyu baÅŸlat
    print("\n" + "="*60)
    print("ğŸš€ LLM API Sunucusu BaÅŸlatÄ±lÄ±yor...")
    print("="*60)
    print(f"ğŸ“Š Model Durumu: {'YÃ¼klÃ¼ âœ…' if LLM_LOADED else 'Demo Modu ğŸ”§'}")
    print(f"ğŸŒ URL: http://localhost:8000")
    print(f"ğŸ“š Docs: http://localhost:8000/docs")
    print("="*60 + "\n")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True
    )
