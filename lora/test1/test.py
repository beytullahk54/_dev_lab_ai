"""
EÄŸitilmiÅŸ LoRA Modelini Test Et
Ä°nteraktif sohbet modu
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

print("\n" + "="*60)
print("ğŸ¤– MODEL TEST - Ä°nteraktif Mod")
print("="*60 + "\n")

# Cihaz kontrolÃ¼
if torch.backends.mps.is_available():
    device = "mps"
    print("âœ… Apple GPU aktif\n")
else:
    device = "cpu"
    print("âš ï¸  CPU kullanÄ±lÄ±yor\n")

# Model yÃ¼kle
print("ğŸ“¦ Model yÃ¼kleniyor...")

base_model = "Qwen/Qwen2.5-1.5B-Instruct"
lora_model = "./lora-model"

tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float32
)

# LoRA adapter'Ä± yÃ¼kle
model = PeftModel.from_pretrained(model, lora_model)
model = model.to(device)
model.eval()

print("âœ… Model hazÄ±r!\n")
print("="*60)
print("ğŸ’¬ Sohbet baÅŸladÄ±! (Ã‡Ä±kmak iÃ§in 'q' yazÄ±n)")
print("="*60 + "\n")

# Ä°nteraktif dÃ¶ngÃ¼
while True:
    prompt = input("ğŸ‘¤ Siz: ")
    
    if prompt.lower() in ['q', 'quit', 'exit', 'Ã§Ä±k']:
        print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!\n")
        break
    
    if not prompt.strip():
        continue
    
    # Model'e formatlÄ± sor (Prompt Template)
    # Model bu formatta eÄŸitildiÄŸi iÃ§in, formatÄ± ona hatÄ±rlatÄ±yoruz.
    full_prompt = f"Soru: {prompt}\nCevap:"
    
    inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=50,      # Cevap Ã§ok uzamasÄ±n
            temperature=0.5,        # Daha tutarlÄ± olsun (0.7 -> 0.5)
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.2  # AynÄ± ÅŸeyi tekrarlamasÄ±nÄ± engelle
        )
    
    # Sadece cevabÄ± al (Soruyu ve prompt'u kes)
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Ã‡Ä±ktÄ±dan sadece "Cevap:" sonrasÄ±nÄ± ayÄ±kla
    if "Cevap:" in full_response:
        response = full_response.split("Cevap:")[-1].strip()
    else:
        response = full_response
        
    print(f"ğŸ¤– Model: {response}\n")
