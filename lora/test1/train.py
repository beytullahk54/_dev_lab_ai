"""
LoRA EÄŸitimi - Apple Silicon iÃ§in Minimal Ã–rnek (DÃœZELTÄ°LDÄ° âœ…)
1B model ile basit eÄŸitim
"""

import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling  # <--- YENÄ°: Bunu ekledik
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset

print("\n" + "="*60)
print("ğŸ LoRA EÄÄ°TÄ°MÄ° - Apple Silicon (Fixed)")
print("="*60 + "\n")

# ============================================
# ADIM 1: CÄ°HAZ KONTROLÃœ
# ============================================
print("ğŸ“± ADIM 1: Cihaz kontrol ediliyor...")

if torch.backends.mps.is_available():
    device = "mps"  # Apple GPU
    print("   âœ… Apple GPU (MPS) aktif!\n")
else:
    device = "cpu"
    print("   âš ï¸  CPU kullanÄ±lacak (daha yavaÅŸ)\n")

# ============================================
# ADIM 2: MODEL YÃœKLEME
# ============================================
print("ğŸ“¦ ADIM 2: Model yÃ¼kleniyor...")
print("   Model: TinyLlama-1.1B")

# Model: Qwen2.5-1.5B (TÃ¼rkÃ§e desteÄŸi Ã‡OK daha iyi)
model_name = "Qwen/Qwen2.5-1.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Padding token ayarla

# Model yÃ¼kle
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,  # Apple iÃ§in en stabil
)
model = model.to(device)

print("   âœ… Model yÃ¼klendi (~1.1B parametre)\n")

# ============================================
# ADIM 3: LORA UYGULA
# ============================================
print("ğŸ”§ ADIM 3: LoRA uygulanÄ±yor...")
print("   LoRA nedir? Modelin sadece kÃ¼Ã§Ã¼k bir kÄ±smÄ±nÄ± eÄŸitir")
print("   Normal: 1.1 milyar parametre eÄŸitilir")
print("   LoRA ile: Sadece ~2 milyon parametre eÄŸitilir!")

lora_config = LoraConfig(
    r=8,              # LoRA rank (ne kadar kÃ¼Ã§Ã¼kse o kadar az parametre)
    lora_alpha=16,    # Scaling factor
    target_modules=["q_proj", "v_proj"],  # Hangi katmanlar eÄŸitilecek
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

print("\n   ğŸ“Š Parametre Ä°statistikleri:")
model.print_trainable_parameters()
print()

# ============================================
# ADIM 4: EÄÄ°TÄ°M VERÄ°SÄ°
# ============================================
print("ğŸ“š ADIM 4: EÄŸitim verisi hazÄ±rlanÄ±yor...")

# Basit TÃ¼rkÃ§e Ã¶rnekler
# Daha Kaliteli ve YapÄ±lÄ± Veri Seti
# Modelin "Soru -> Cevap" iliÅŸkisini kurabilmesi iÃ§in formatlÄ± veri kullanÄ±yoruz.
train_data = [
    "Soru: Merhaba.\nCevap: Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?",
    "Soru: NasÄ±lsÄ±n?\nCevap: TeÅŸekkÃ¼r ederim, iyiyim. Siz nasÄ±lsÄ±nÄ±z?",
    "Soru: Python nedir?\nCevap: Python, Ã¶ÄŸrenmesi kolay ve Ã§ok popÃ¼ler bir programlama dilidir.",
    "Soru: TÃ¼rkiye'nin baÅŸkenti neresidir?\nCevap: TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r.",
    "Soru: Ä°stanbul'un nÃ¼fusu kaÃ§tÄ±r?\nCevap: Ä°stanbul, TÃ¼rkiye'nin en kalabalÄ±k ÅŸehridir.",
    "Soru: Yapay zeka nedir?\nCevap: Yapay zeka, bilgisayarlarÄ±n insan gibi dÃ¼ÅŸÃ¼nmesini saÄŸlayan teknolojidir.",
    "Soru: LoRA ne iÅŸe yarar?\nCevap: LoRA, bÃ¼yÃ¼k yapay zeka modellerini Ã§ok daha az bellek ve iÅŸlem gÃ¼cÃ¼yle eÄŸitmemizi saÄŸlar.",
    "Soru: Apple Silicon (M1/M2) iyi midir?\nCevap: Evet, Apple Silicon iÅŸlemciler hem Ã§ok hÄ±zlÄ±dÄ±r hem de Ã§ok az enerji tÃ¼ketir.",
    "Soru: Derin Ã¶ÄŸrenme nedir?\nCevap: Derin Ã¶ÄŸrenme, insan beynindeki sinir aÄŸlarÄ±nÄ± taklit eden bir yapay zeka yÃ¶ntemidir.",
    "Soru: En iyi programlama dili hangisi?\nCevap: Projeye gÃ¶re deÄŸiÅŸir ama Python, JavaScript ve C++ en popÃ¼ler dillerdendir.",
    "Soru: YazÄ±lÄ±m Ã¶ÄŸrenmek zor mu?\nCevap: BaÅŸlarda zorlayÄ±cÄ± olabilir ama sabÄ±r ve pratikle herkes yazÄ±lÄ±m Ã¶ÄŸrenebilir.",
    "Soru: BugÃ¼n hava nasÄ±l?\nCevap: Ben bir yapay zekayÄ±m, dÃ¼nyadaki hava durumunu gÃ¶remem ama umarÄ±m hava gÃ¼zeldir.",
    "Soru: Bana bir fÄ±kra anlat.\nCevap: Temel bir gÃ¼n... Åaka ÅŸaka, ben daha Ã§ok teknik konularda yardÄ±mcÄ± olabilirim.",
    "Soru: Bilgisayar nedir?\nCevap: Bilgisayar, verileri iÅŸleyen ve saklayan elektronik bir cihazdÄ±r.",
    "Soru: Ä°nternet nasÄ±l Ã§alÄ±ÅŸÄ±r?\nCevap: Ä°nternet, dÃ¼nya genelindeki bilgisayarlarÄ±n birbirine kablolar ve sinyallerle baÄŸlandÄ±ÄŸÄ± dev bir aÄŸdÄ±r."
]

print(f"   âœ… {len(train_data)} Ã¶rnek hazÄ±rlandÄ±\n")

# Dataset oluÅŸtur ve tokenize et
dataset = Dataset.from_dict({"text": train_data})

def tokenize(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=128,
        # padding burada yapmÄ±yoruz, DataCollator yapacak
    )

tokenized_dataset = dataset.map(tokenize, batched=True)

# <--- YENÄ°: Data Collator (Labels/Cevap AnahtarÄ± oluÅŸturur)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, 
    mlm=False  # Masked Language Modeling deÄŸil, Causal LM yapÄ±yoruz
)

print(f"   âœ… {len(train_data)} Ã¶rnek hazÄ±rlandÄ±\n")

# ============================================
# ADIM 5: EÄÄ°TÄ°M AYARLARI
# ============================================
print("âš™ï¸  ADIM 5: EÄŸitim ayarlarÄ± yapÄ±lÄ±yor...")

training_args = TrainingArguments(
    output_dir="./lora-model",           # Model nereye kaydedilecek
    num_train_epochs=10,                  # Daha iyi Ã¶ÄŸrenmesi iÃ§in artÄ±rdÄ±k (3 -> 10)
    per_device_train_batch_size=2,       # AynÄ± anda kaÃ§ Ã¶rnek
    learning_rate=2e-4,                   # Ã–ÄŸrenme hÄ±zÄ±
    logging_steps=1,                      # Her adÄ±mda log gÃ¶ster
    save_steps=50,                        # Her 50 adÄ±mda kaydet
    save_total_limit=1,                   # Sadece en iyi modeli sakla
    report_to="none",                     # LoglarÄ± gÃ¶sterme
    dataloader_pin_memory=False,          # Apple hatasÄ±nÄ± Ã¶nlemek iÃ§in
)

# ============================================
# ADIM 6: EÄÄ°TÄ°M
# ============================================
print("ğŸ“ ADIM 6: EÄŸitim baÅŸlÄ±yor...")
print("   â±ï¸  Tahmini sÃ¼re: 2-5 dakika\n")
print("-" * 60)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator, # <--- YENÄ°: Collator'Ä± ekledik
)

# EÄÄ°TÄ°MÄ° BAÅLAT!
trainer.train()

print("-" * 60)
print("\n   âœ… EÄŸitim tamamlandÄ±!\n")

# ============================================
# ADIM 7: MODELÄ° KAYDET
# ============================================
print("ğŸ’¾ ADIM 7: Model kaydediliyor...")

model.save_pretrained("./lora-model")
tokenizer.save_pretrained("./lora-model")

print("   âœ… Model kaydedildi: ./lora-model\n")

# ============================================
# ADIM 8: TEST
# ============================================
print("ğŸ§ª ADIM 8: HÄ±zlÄ± test yapÄ±lÄ±yor...\n")

model.eval()
test_prompt = "Merhaba!"

inputs = tokenizer(test_prompt, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model.generate(
        **inputs, 
        max_new_tokens=30, 
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"   Prompt: {test_prompt}")
print(f"   YanÄ±t: {response}\n")

# ============================================
# BÄ°TÄ°Å
# ============================================
print("="*60)
print("ğŸ‰ TAMAMLANDI!")
print("="*60)
print("\nğŸ“ OluÅŸturulan dosyalar:")
print("   â€¢ ./lora-model/  (EÄŸitilmiÅŸ model)")
print("\nğŸš€ Sonraki adÄ±mlar:")
print("   â€¢ Modeli test et: python test.py")
print("   â€¢ Daha fazla veri ekle ve tekrar eÄŸit")
print("\nğŸ’¡ LoRA'yÄ± Ã¶ÄŸrendiniz!")
print("   â€¢ Sadece 2M parametre eÄŸittiniz (1.1B yerine)")
print("   â€¢ %99.8 daha az bellek kullandÄ±nÄ±z")
print("   â€¢ Ã‡ok daha hÄ±zlÄ± eÄŸittiniz")
print("="*60 + "\n")
