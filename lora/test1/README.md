# LoRA EÄŸitimi - Basit BaÅŸlangÄ±Ã§ ğŸ

Apple Silicon iÃ§in optimize edilmiÅŸ, **Ã§ok basit** LoRA eÄŸitim projesi.

## ğŸ¯ LoRA Nedir?

**Normal EÄŸitim:**

- 1.1 milyar parametre eÄŸitilir
- Ã‡ok fazla bellek gerekir (16GB+)
- Ã‡ok yavaÅŸ

**LoRA ile:**

- Sadece 2 milyon parametre eÄŸitilir (%99.8 daha az!)
- Az bellek gerekir (4-8GB)
- Ã‡ok hÄ±zlÄ±

## ğŸ“‹ Gereksinimler

- **Mac:** M1, M2 veya M3 chip
- **RAM:** En az 8GB
- **Python:** 3.8+

## ğŸš€ Kurulum (3 AdÄ±m)

### 1. KÃ¼tÃ¼phaneleri YÃ¼kle

```bash
pip install -r requirements.txt
```

â±ï¸ Ä°lk kurulum 5-10 dakika sÃ¼rebilir.

### 2. EÄŸitimi BaÅŸlat

```bash
python train.py
```

â±ï¸ EÄŸitim 2-5 dakika sÃ¼rer.

### 3. Modeli Test Et

```bash
python test.py
```

## ğŸ“ Dosyalar

```
.
â”œâ”€â”€ requirements.txt    # Gerekli kÃ¼tÃ¼phaneler (5 adet)
â”œâ”€â”€ train.py           # EÄŸitim scripti (8 adÄ±m)
â”œâ”€â”€ test.py            # Test scripti (interaktif)
â””â”€â”€ README.md          # Bu dosya
```

## ğŸ“– Kod AÃ§Ä±klamasÄ±

### train.py - 8 Basit AdÄ±m

```python
# ADIM 1: Cihaz kontrolÃ¼ (Apple GPU var mÄ±?)
device = "mps" if torch.backends.mps.is_available() else "cpu"

# ADIM 2: 1B model yÃ¼kle
model = AutoModelForCausalLM.from_pretrained("TinyLlama-1.1B")

# ADIM 3: LoRA uygula (1.1B â†’ 2M parametre!)
lora_config = LoraConfig(r=8)
model = get_peft_model(model, lora_config)

# ADIM 4: EÄŸitim verisi hazÄ±rla
train_data = ["Merhaba!", "Python nedir?", ...]

# ADIM 5: EÄŸitim ayarlarÄ±
training_args = TrainingArguments(num_train_epochs=3, ...)

# ADIM 6: EÄŸit!
trainer = Trainer(model=model, ...)
trainer.train()

# ADIM 7: Kaydet
model.save_pretrained("./lora-model")

# ADIM 8: Test
outputs = model.generate(inputs)
```

## âš™ï¸ Parametreler

### LoRA Rank (r)

```python
r=8   # HÄ±zlÄ±, az parametre (baÅŸlangÄ±Ã§ iÃ§in iyi) âœ…
r=16  # Daha iyi sonuÃ§lar
r=32  # En iyi sonuÃ§lar ama yavaÅŸ
```

### Epoch SayÄ±sÄ±

```python
num_train_epochs=3  # Normal âœ…
num_train_epochs=5  # Daha iyi Ã¶ÄŸrenir
num_train_epochs=1  # HÄ±zlÄ± test iÃ§in
```

### Batch Size

```python
per_device_train_batch_size=2  # Az bellek âœ…
per_device_train_batch_size=4  # Daha hÄ±zlÄ± (M2/M3 iÃ§in)
```

## ğŸ“ Kendi Verinizle EÄŸitin

### train.py iÃ§inde deÄŸiÅŸtirin:

```python
# Eski:
train_data = [
    "Merhaba! NasÄ±lsÄ±n?",
    "Python nedir?",
]

# Yeni:
train_data = [
    "Kendi cÃ¼mlelerinizi buraya yazÄ±n",
    "Ne kadar Ã§ok veri o kadar iyi",
    "En az 50-100 cÃ¼mle olmalÄ±",
    # ... daha fazla
]
```

veya dosyadan yÃ¼kleyin:

```python
with open("veriler.txt", "r", encoding="utf-8") as f:
    train_data = f.readlines()
```

## ğŸ“Š Performans

| Mac | EÄŸitim SÃ¼resi (8 Ã¶rnek) | GerÃ§ek Veri (1000 Ã¶rnek) |
| --- | ----------------------- | ------------------------ |
| M1  | 3-5 dakika              | 20-30 dakika             |
| M2  | 2-4 dakika              | 15-20 dakika             |
| M3  | 2-3 dakika              | 10-15 dakika             |

## ğŸ› Sorun Giderme

### "MPS backend not available"

```bash
# PyTorch gÃ¼ncelleyin:
pip install --upgrade torch
```

### "Out of memory"

```python
# train.py iÃ§inde:
per_device_train_batch_size=1  # 2 yerine 1
max_length=64  # 128 yerine 64
```

### Model saÃ§ma yanÄ±tlar veriyor

- Daha fazla veri ekleyin (100+)
- Daha fazla epoch kullanÄ±n (5-10)
- r deÄŸerini artÄ±rÄ±n (16 veya 32)

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Ä°lk deneme:** Kodu olduÄŸu gibi Ã§alÄ±ÅŸtÄ±rÄ±n
2. **Veri ekleyin:** 50-100 Ã¶rnek ekleyin
3. **Parametrelerle oynayÄ±n:** r, epochs, batch_size
4. **Test edin:** Her deÄŸiÅŸiklikten sonra test edin

## â“ SSS

**S: GPU ÅŸart mÄ±?**  
C: HayÄ±r ama Apple GPU (MPS) Ã§ok daha hÄ±zlÄ±. CPU ile de Ã§alÄ±ÅŸÄ±r.

**S: KaÃ§ veri gerekli?**  
C: Minimum 10-20, ideal 100-1000+

**S: EÄŸitim ne kadar sÃ¼rer?**  
C: 8 Ã¶rnek iÃ§in 2-5 dakika, 1000 Ã¶rnek iÃ§in 15-30 dakika

**S: Model ne kadar yer kaplar?**  
C: LoRA adapter sadece 10-20MB! Base model 2GB ama bir kez indirilir.

**S: TÃ¼rkÃ§e Ã§alÄ±ÅŸÄ±r mÄ±?**  
C: Evet! TinyLlama Ä°ngilizce ama TÃ¼rkÃ§e de Ã¶ÄŸrenir.

## ğŸ‰ BaÅŸarÄ±lar!

ArtÄ±k LoRA'nÄ±n temellerini Ã¶ÄŸrendiniz:

- âœ… %99.8 daha az parametre eÄŸittiniz
- âœ… Ã‡ok daha hÄ±zlÄ± eÄŸittiniz
- âœ… Kendi modelinizi oluÅŸturdunuz

SorularÄ±nÄ±z iÃ§in issue aÃ§abilirsiniz!
