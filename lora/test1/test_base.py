"""
TEST BASE - EÄŸitilmemiÅŸ Model Testi
LoRA modelini YÃœKLEMEDEN ham modelin nasÄ±l cevap verdiÄŸini test edin.
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

print("\n" + "="*60)
print("ğŸ‘¶ TEST BASE - EÄŸitilmemiÅŸ Ham Model")
print("="*60 + "\n")

# Cihaz kontrolÃ¼
if torch.backends.mps.is_available():
    device = "mps"
    print("âœ… Apple GPU aktif\n")
else:
    device = "cpu"
    print("âš ï¸  CPU kullanÄ±lÄ±yor\n")

# Model yÃ¼kle
print("ğŸ“¦ Ham model yÃ¼kleniyor (TinyLlama-1.1B)...")
print("âš ï¸  UyarÄ±: Bu model LoRA adaptÃ¶rÃ¼nÃ¼ KULLANMIYOR!")

model_name = "Qwen/Qwen2.5-1.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32
)
model = model.to(device)
model.eval()

print("âœ… Ham model hazÄ±r!\n")
print("="*60)
print("ğŸ’¬ Sohbet baÅŸladÄ±! (Ã‡Ä±kmak iÃ§in 'q' yazÄ±n)")
print("ğŸ‘‰ TÃ¼rkÃ§e soru sorduÄŸunuzda muhtemelen Ä°ngilizce cevap verecek veya saÃ§malayacaktÄ±r.")
print("="*60 + "\n")

# Ä°nteraktif dÃ¶ngÃ¼
while True:
    prompt = input("ğŸ‘¤ Siz (Ham Modele Soruyorsunuz): ")
    
    if prompt.lower() in ['q', 'quit', 'exit', 'Ã§Ä±k']:
        print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!\n")
        break
    
    if not prompt.strip():
        continue
    
    # Model'e sor
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id  # Base model eos kullanÄ±r
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ğŸ¤– Ham Model: {response}\n")
